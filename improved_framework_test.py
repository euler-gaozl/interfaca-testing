#!/usr/bin/env python3
"""
æ”¹è¿›çš„æ™ºèƒ½ä½“æ¡†æ¶æµ‹è¯•è„šæœ¬
å¢å¼ºé”™è¯¯å¤„ç†å’Œè¯Šæ–­åŠŸèƒ½
"""

import asyncio
import time
import json
from datetime import datetime
from typing import Dict, Any, List
import traceback

from src.agents.test_generator import TestCaseGeneratorAgent
from src.agents.langchain_agent import LangChainTestAgent
from src.agents.autogen_agent import AutoGenMultiAgent
from src.utils.logger import log


class ImprovedFrameworkTester:
    """æ”¹è¿›çš„æ¡†æ¶æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_api_spec = {
            "openapi": "3.0.0",
            "info": {
                "title": "ç”µå•†API",
                "version": "1.0.0"
            },
            "paths": {
                "/products": {
                    "get": {
                        "summary": "è·å–å•†å“åˆ—è¡¨",
                        "parameters": [
                            {
                                "name": "category",
                                "in": "query",
                                "schema": {"type": "string"}
                            },
                            {
                                "name": "limit",
                                "in": "query",
                                "schema": {"type": "integer"}
                            }
                        ],
                        "responses": {
                            "200": {"description": "æˆåŠŸ"}
                        }
                    },
                    "post": {
                        "summary": "åˆ›å»ºå•†å“",
                        "requestBody": {
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "type": "object",
                                        "properties": {
                                            "name": {"type": "string"},
                                            "price": {"type": "number"},
                                            "category": {"type": "string"}
                                        },
                                        "required": ["name", "price"]
                                    }
                                }
                            }
                        },
                        "responses": {
                            "201": {"description": "åˆ›å»ºæˆåŠŸ"}
                        }
                    }
                },
                "/orders": {
                    "post": {
                        "summary": "åˆ›å»ºè®¢å•",
                        "security": [{"bearerAuth": []}],
                        "responses": {
                            "201": {"description": "è®¢å•åˆ›å»ºæˆåŠŸ"}
                        }
                    }
                }
            }
        }
        
        self.test_requirements = {
            "test_types": ["functional", "security"],
            "coverage": "comprehensive",
            "priority": "high"
        }
    
    async def test_framework(self, framework_name: str, agent_class, timeout: int = 180) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæ¡†æ¶"""
        print(f"\nğŸ§ª æµ‹è¯• {framework_name}")
        print("=" * 60)
        
        start_time = time.time()
        result = {
            "framework": framework_name,
            "success": False,
            "test_cases": [],
            "duration": 0,
            "error": None,
            "details": {}
        }
        
        try:
            # åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
            print(f"   ğŸ“‹ åˆå§‹åŒ–{framework_name}æ™ºèƒ½ä½“...")
            agent = agent_class()
            
            # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
            if hasattr(agent, 'use_mock') and agent.use_mock:
                print(f"   âš ï¸  {framework_name}ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                result["details"]["mock_mode"] = True
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = {
                "api_spec": self.test_api_spec,
                "test_types": self.test_requirements["test_types"],
                "requirements": self.test_requirements,
                "project_id": 1  # æ·»åŠ å¿…éœ€çš„project_idå‚æ•°
            }
            
            print(f"   ğŸš€ å¯åŠ¨{framework_name}å¤„ç†...")
            
            # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
            try:
                response = await asyncio.wait_for(
                    agent.process(input_data),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                raise Exception(f"å¤„ç†è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
            
            # å¤„ç†ç»“æœ
            if response and response.get("success", False):
                test_cases = response.get("test_cases", [])
                result.update({
                    "success": True,
                    "test_cases": test_cases,
                    "test_case_count": len(test_cases),
                    "response": response
                })
                
                # æå–æ¡†æ¶ç‰¹å®šä¿¡æ¯
                if framework_name == "LangChain":
                    result["details"]["tools_used"] = response.get("tools_used", [])
                    result["details"]["langchain_output"] = response.get("langchain_output", "")
                elif framework_name == "AutoGen":
                    result["details"]["agents_involved"] = response.get("agents_involved", [])
                    result["details"]["collaboration_messages"] = response.get("collaboration_messages", [])
                
                print(f"   âœ… {framework_name}: æˆåŠŸç”Ÿæˆ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
                
            else:
                error_msg = response.get("error", "æœªçŸ¥é”™è¯¯") if response else "æ— å“åº”"
                result["error"] = error_msg
                print(f"   âŒ {framework_name}: å¤„ç†å¤±è´¥ - {error_msg}")
            
        except Exception as e:
            result["error"] = str(e)
            result["traceback"] = traceback.format_exc()
            print(f"   ğŸ’¥ {framework_name}: å¼‚å¸¸ - {e}")
            log.error(f"{framework_name}æµ‹è¯•å¼‚å¸¸: {e}")
            log.debug(f"{framework_name}å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        
        finally:
            result["duration"] = time.time() - start_time
            print(f"   â±ï¸  è€—æ—¶: {result['duration']:.2f} ç§’")
        
        return result
    
    def calculate_quality_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†"""
        if not result["success"]:
            return 0.0
        
        score = 0.0
        test_cases = result.get("test_cases", [])
        
        # åŸºç¡€åˆ†æ•°
        if test_cases:
            score += 3.0
        
        # æµ‹è¯•ç”¨ä¾‹æ•°é‡åˆ†æ•°
        case_count = len(test_cases)
        if case_count >= 3:
            score += 2.0
        elif case_count >= 2:
            score += 1.5
        elif case_count >= 1:
            score += 1.0
        
        # æµ‹è¯•ç±»å‹è¦†ç›–åˆ†æ•°
        test_types = set()
        for case in test_cases:
            test_types.add(case.get("test_type", "unknown"))
        
        if "functional" in test_types:
            score += 1.0
        if "security" in test_types:
            score += 1.5
        if "performance" in test_types:
            score += 1.0
        
        # æ¡†æ¶ç‰¹å®šåŠ åˆ†
        framework = result["framework"]
        if framework == "LangChain":
            tools_used = result.get("details", {}).get("tools_used", [])
            if len(tools_used) > 1:
                score += 0.5
        elif framework == "AutoGen":
            agents = result.get("details", {}).get("agents_involved", [])
            if len(agents) > 2:
                score += 0.5
        
        # æ€§èƒ½åŠ åˆ†
        duration = result.get("duration", 0)
        if duration < 30:
            score += 0.5
        elif duration < 60:
            score += 0.3
        
        return min(score, 10.0)
    
    def get_framework_features(self, framework: str) -> List[str]:
        """è·å–æ¡†æ¶ç‰¹æ€§"""
        features = {
            "DeepSeek": ["æ€è€ƒæ¨ç†", "ä¸­æ–‡ä¼˜åŒ–", "æœ¬åœ°éƒ¨ç½²", "å¿«é€Ÿå“åº”"],
            "LangChain": ["å·¥å…·é“¾ä¸°å¯Œ", "æ¨¡å—åŒ–è®¾è®¡", "å¯æ‰©å±•æ€§", "ç”Ÿæ€å®Œå–„"],
            "AutoGen": ["å¤šæ™ºèƒ½ä½“åä½œ", "ä¸“ä¸šåˆ†å·¥", "ç¾¤ä½“æ™ºèƒ½", "è§’è‰²æ‰®æ¼”"]
        }
        return features.get(framework, ["æœªçŸ¥ç‰¹æ€§"])
    
    async def run_comprehensive_test(self):
        """è¿è¡Œå…¨é¢æµ‹è¯•"""
        print("ğŸ¯ æ”¹è¿›çš„æ™ºèƒ½ä½“æ¡†æ¶å¯¹æ¯”æµ‹è¯•")
        print("=" * 80)
        
        # æµ‹è¯•æ¡†æ¶é…ç½®
        frameworks = [
            ("DeepSeek", TestCaseGeneratorAgent),
            ("LangChain", LangChainTestAgent),
            ("AutoGen", AutoGenMultiAgent)
        ]
        
        results = []
        
        # é€ä¸ªæµ‹è¯•æ¡†æ¶
        for framework_name, agent_class in frameworks:
            result = await self.test_framework(framework_name, agent_class)
            result["quality_score"] = self.calculate_quality_score(result)
            result["features"] = self.get_framework_features(framework_name)
            results.append(result)
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(results)
        
        return results
    
    def generate_comparison_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\nğŸ“Š æµ‹è¯•ç»“æœå¯¹æ¯”åˆ†æ")
        print("=" * 80)
        
        # æˆåŠŸç‡ç»Ÿè®¡
        successful = [r for r in results if r["success"]]
        success_rate = len(successful) / len(results) * 100
        
        print(f"\nğŸ“‹ æˆåŠŸç‡ç»Ÿè®¡")
        print("-" * 60)
        print(f"   æ€»æµ‹è¯•æ¡†æ¶: {len(results)}")
        print(f"   æˆåŠŸæ¡†æ¶: {len(successful)}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æ€§èƒ½å¯¹æ¯”
        print(f"\nğŸ“‹ æ€§èƒ½å¯¹æ¯”")
        print("-" * 60)
        for result in successful:
            duration = result["duration"]
            case_count = len(result["test_cases"])
            print(f"   ğŸš€ {result['framework']}: {duration:.2f}ç§’, {case_count}ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # ç‰¹æ€§å¯¹æ¯”
        print(f"\nğŸ“‹ ç‰¹æ€§å¯¹æ¯”")
        print("-" * 60)
        for result in results:
            features = ", ".join(result["features"])
            print(f"   ğŸ¯ {result['framework']}: {features}")
        
        # è´¨é‡å¯¹æ¯”
        print(f"\nğŸ“‹ è´¨é‡å¯¹æ¯”")
        print("-" * 60)
        for result in successful:
            score = result["quality_score"]
            print(f"   ğŸ† {result['framework']}: è´¨é‡è¯„åˆ† {score:.1f}/10.0")
        
        # é”™è¯¯åˆ†æ
        failed = [r for r in results if not r["success"]]
        if failed:
            print(f"\nğŸ“‹ é”™è¯¯åˆ†æ")
            print("-" * 60)
            for result in failed:
                print(f"   âŒ {result['framework']}: {result['error']}")
        
        # æ¨èå»ºè®®
        if successful:
            best = max(successful, key=lambda x: x["quality_score"])
            print(f"\nğŸ“‹ æ¡†æ¶æ¨è")
            print("-" * 60)
            print(f"   ğŸ¯ æ¨èä½¿ç”¨: {best['framework']}")
            print(f"   ğŸ“Š è´¨é‡è¯„åˆ†: {best['quality_score']:.1f}/10.0")
            print(f"   âš¡ æ€§èƒ½è¡¨ç°: {best['duration']:.2f}ç§’")
            print(f"   ğŸ§ª æµ‹è¯•ç”¨ä¾‹: {len(best['test_cases'])}ä¸ª")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"improved_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“‹ ä¿å­˜æµ‹è¯•ç»“æœ")
        print("-" * 60)
        print(f"   âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")


async def main():
    """ä¸»å‡½æ•°"""
    tester = ImprovedFrameworkTester()
    
    try:
        results = await tester.run_comprehensive_test()
        
        print("\nğŸ‰ æ”¹è¿›æµ‹è¯•å®Œæˆæ€»ç»“")
        print("=" * 80)
        
        successful_count = sum(1 for r in results if r["success"])
        total_cases = sum(len(r["test_cases"]) for r in results)
        
        print(f"âœ¨ æ™ºèƒ½ä½“æ¡†æ¶æ”¹è¿›æµ‹è¯•å®Œæˆï¼")
        print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   â€¢ æµ‹è¯•æ¡†æ¶: {len(results)} ä¸ª")
        print(f"   â€¢ æˆåŠŸæ¡†æ¶: {successful_count} ä¸ª")
        print(f"   â€¢ æ€»æµ‹è¯•ç”¨ä¾‹: {total_cases} ä¸ª")
        
        if successful_count > 0:
            best = max([r for r in results if r["success"]], key=lambda x: x["quality_score"])
            print(f"\nğŸ† æœ€ä½³æ¡†æ¶: {best['framework']}")
            print(f"   â€¢ è´¨é‡è¯„åˆ†: {best['quality_score']:.1f}/10.0")
            print(f"   â€¢ æ‰§è¡Œæ—¶é—´: {best['duration']:.2f}ç§’")
            print(f"   â€¢ æµ‹è¯•ç”¨ä¾‹: {len(best['test_cases'])}ä¸ª")
        
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"   â€¢ å¿«é€ŸåŸå‹: é€‰æ‹©DeepSeekåŸºç¡€æ¡†æ¶")
        print(f"   â€¢ å¤æ‚åˆ†æ: é€‰æ‹©LangChainå·¥å…·é“¾")
        print(f"   â€¢ å›¢é˜Ÿåä½œ: é€‰æ‹©AutoGenå¤šæ™ºèƒ½ä½“")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        log.error(f"ä¸»æµ‹è¯•æµç¨‹é”™è¯¯: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
